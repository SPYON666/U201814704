# -*- coding: UTF-8 -*-
import os
import random
from sklearn.naive_bayes import MultinomialNB
import matplotlib.pyplot as plt
import re
'''
处理流程：
一、文本文件读取处理
    将文本依次读入，（去除标点符号和小写化，分词后转化为list格式），并产生对应的数据集和标签集
    将读取到的集按顺序打乱，并划分为训练集，测试集
    对训练集进行处理
        统计词频，从而得到词频表，用来产生特征词汇表，方便对数据集进行特征化
'''
def TextProcessing(folder_path, test_size=0.2):#有一个大文件夹，内含不同类别的文件夹，每个文件夹内有txt文件
    folder_list = os.listdir(folder_path)  # 查看folder_path下的文件
    data_list = []  # 数据集数据
    class_list = []  # 数据集类别

    # 遍历每个子文件夹
    for folder in folder_list:#folder是一个同类txt组成的文件夹
        new_folder_path = os.path.join(folder_path, folder)  # 根据子文件夹，生成新的路径
        files = os.listdir(new_folder_path)  # 存放子文件夹下的txt文件的列表

        j = 1
        # 遍历每个txt文件
        for file in files:#每个文件夹下的txt读取100个
            if j > 100:  # 每类txt样本数最多100个
                break
            with open(os.path.join(new_folder_path, file), 'r') as f:  # 打开txt文件
                raw = f.read()

            word_list = re.split(r'\W+', raw)  # 将文本字符串切分并去除特殊符号，形成字符列表
            word_list = [word.lower() for word in word_list if len(word) >= 2]  # 小写化

            data_list.append(word_list)  # 添加数据集数据
            class_list.append(folder)  # 添加数据集类别
            j += 1

    data_class_list = list(zip(data_list, class_list))  # zip压缩合并，将数据与标签对应压缩  #每个txt的数据与对应的类别压缩
    # 将原来所有的txt文件 抽象为一个个 1个标签加txt内的词汇合在一起的列表成员
    random.shuffle(data_class_list)  # 将data_class_list乱序
    index = int(len(data_class_list) * test_size) + 1  # 训练集和测试集切分的索引值
    train_list = data_class_list[index:]  # 训练集
    test_list = data_class_list[:index]  # 测试集
    train_data_list, train_class_list = zip(*train_list)  # 训练集解压缩 #将原来的压缩形式变成两个列表，标签成员与每个文本的全部词汇一一对应
    test_data_list, test_class_list = zip(*test_list)  # 测试集解压缩，train_data_list内含每一个文本的全部词汇

    all_words_dict = {}  # 统计训练集词频，形成一个词汇名为key，出现次数为value的字典
    for word_list in train_data_list:
        for word in word_list:
            if word in all_words_dict.keys():
                all_words_dict[word] += 1
            else:
                all_words_dict[word] = 1

    # 根据键的值倒序排序
    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f: f[1], reverse=True)#.items会形成（key,value）的元组的列表，从大到小的倒序排序
    #lamba 为匿名函数，可以看做表达式，放入f，返回f[1],key代表用来进行排序的值是什么
    all_words_list, all_words_nums = zip(*all_words_tuple_list)  # 解压缩
    all_words_list = list(all_words_list)  # 转换成列表all_words_list，内含训练集中出现过的所有词汇，并且出现次数最多的在第一个

    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list

'''
处理流程：
二、特征化训练集
    根据词频表生成特征词汇表
    将所有的数据集进行处理，将乱序的词汇列表转换为特征向量的形式，从而能够进行sklearn的贝叶斯模型训练
    这是对于本次实验最重要的过程，也是影响判读准确率的各种变量出现最多的地方，同时也是可调节度最大的地方
'''
def TextFeatures(train_data_list, test_data_list,all_words_list, deleteN):
    feature_words = []  # 特征列表
    n = 1
    for t in range(deleteN, len(all_words_list), 1):
        if n > 1000:  # feature_words的维度为1000
            break
            # 如果这个词不是数字，并且单词长度大于1小于5，那么这个词就可以作为特征词
        if not all_words_list[t].isdigit() and  1 < len(all_words_list[t]) < 10:
            feature_words.append(all_words_list[t])
        n += 1
    def text_features(text, feature_words):  # 出现在特征集中，则置1
        text_words = set(text) #text代表一个文本文件的全部词汇，现将其去重，代表含有多少种词
        features = [1 if word in text_words else 0 for word in feature_words]#产生与text对应的特征词汇表类型的版本，
        return features
    train_feature_list = [text_features(text, feature_words) for text in train_data_list]#将原来的每一个文本文件成员都转换为对应的特征词汇表
    test_feature_list = [text_features(text, feature_words) for text in test_data_list]
    return train_feature_list, test_feature_list  # 返回结果
'''
处理流程：
三、利用sklearn的贝叶斯概率模型生成分类器
    将已经特征化处理的数据集和标签集放入MultinomialNB().fit进行训练，并将测试集进行分类并对比，判断准确率
    sklearn的贝叶斯模型更加优化，计算效率更高，同时也更加方便
'''

def TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list):
    classifier = MultinomialNB().fit(train_feature_list, train_class_list)#根据训练集和标签产生分类器
    test_accuracy = classifier.score(test_feature_list, test_class_list)#利用分类器进行对比
    #多项式模型，即利用训练集产生不同类型的邮件的词汇骰子，以此判断测试集为那一类
    return test_accuracy


if __name__ == '__main__':
    # 文本预处理
    folder_path = './email'  # 训练集存放地址

    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path,test_size=0.5)
    print("读取数据结束\n\n\n")
    print("当前训练集中共出现了%d个不同词汇（忽略大小写，抛去标点符号）"%(len(all_words_list)))
    print("以抛去前n个高频词汇为自变量》》》》观察其对测试正确性的影响")
    test_accuracy_list = []
    deleteNs = range(0,len(all_words_list)-1, 1)  # 0 20 40 60 ... 980
    for deleteN in deleteNs:
        train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list,all_words_list, deleteN,)
        #利用特征词集合，将训练集和测试集的每一个文本成员转换为特征词是否出现的特征向量成员,即train_feature_list
        if len(train_feature_list[0]) == 0:
            print("由于删除词汇的影响导致无特征向量词汇产生，故跳过")
            test_accuracy_list.append(0)#当特征向量表内没有特征词汇时，选择跳过该次处理，同时以0作为判断结果
            break
        test_accuracy = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list)
        print("抛去前%d个高频词汇的影响，得到的测试正确率为》》》》"%(deleteN),test_accuracy)
        #利用sklearn自带的朴素贝叶斯的多项式模型处理数据并猜测，返回判断精度
        test_accuracy_list.append(test_accuracy)
    plt.figure()
    plt.plot(deleteNs, test_accuracy_list)
    plt.title('Relationship of deleteNs and test_accuracy')
    plt.xlabel('deleteNs')
    plt.ylabel('test_accuracy')
    plt.show()

